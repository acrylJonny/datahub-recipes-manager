name: E2E Selenium Tests

on:
  push:
    branches: [ main ]
    paths:
      - 'web_ui/**'
      - 'utils/**'
      - 'metadata-manager/**'
      - '.github/workflows/test-e2e-selenium.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'web_ui/templates/**'
      - 'web_ui/static/**'
      - 'web_ui/metadata_manager/**'
      - 'web_ui/tests/frontend/**'
    types: [opened, synchronize, reopened]
  workflow_dispatch:
    inputs:
      test_class:
        description: 'Specific test class to run (optional)'
        required: false
        default: ''
      verbose:
        description: 'Run with verbose output'
        required: false
        default: false
        type: boolean

jobs:
  selenium-e2e-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            xvfb \
            firefox \
            chromium-browser \
            chromium-chromedriver \
            postgresql-client

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -r web_ui/requirements.txt
          pip install selenium pytest-xvfb pytest-django factory-boy

      - name: Set up test environment
        working-directory: web_ui
        run: |
          echo "Setting up Django test environment..."
          export DJANGO_SETTINGS_MODULE=test_settings
          python manage.py collectstatic --noinput --settings=test_settings
          python manage.py migrate --settings=test_settings

      - name: Start virtual display
        run: |
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
          sleep 3

      - name: Run Selenium E2E Tests - Core Pages
        working-directory: web_ui
        env:
          DISPLAY: :99
          DJANGO_SETTINGS_MODULE: test_settings
        run: |
          echo "Running core metadata page tests..."
          python manage.py test tests.frontend.test_metadata_selenium.TagsPageSeleniumTestCase \
            tests.frontend.test_metadata_selenium.DomainsPageSeleniumTestCase \
            tests.frontend.test_metadata_selenium.GlossaryPageSeleniumTestCase \
            --keepdb --verbosity=${{ github.event.inputs.verbose == 'true' && '2' || '1' }}

      - name: Run Selenium E2E Tests - Data Products & Assertions
        working-directory: web_ui
        env:
          DISPLAY: :99
          DJANGO_SETTINGS_MODULE: test_settings
        run: |
          echo "Running data products and assertions tests..."
          python manage.py test tests.frontend.test_metadata_selenium.DataProductsPageSeleniumTestCase \
            tests.frontend.test_metadata_selenium.AssertionsPageSeleniumTestCase \
            --keepdb --verbosity=${{ github.event.inputs.verbose == 'true' && '2' || '1' }}

      - name: Run Selenium E2E Tests - Integration Workflows
        working-directory: web_ui
        env:
          DISPLAY: :99
          DJANGO_SETTINGS_MODULE: test_settings
        run: |
          echo "Running integration workflow tests..."
          python manage.py test tests.frontend.test_metadata_selenium.MetadataWorkflowIntegrationTestCase \
            --keepdb --verbosity=${{ github.event.inputs.verbose == 'true' && '2' || '1' }}

      - name: Run Custom Test Class (if specified)
        if: github.event.inputs.test_class != ''
        working-directory: web_ui
        env:
          DISPLAY: :99  
          DJANGO_SETTINGS_MODULE: test_settings
        run: |
          echo "Running custom test class: ${{ github.event.inputs.test_class }}"
          python manage.py test ${{ github.event.inputs.test_class }} \
            --keepdb --verbosity=2

      - name: Upload screenshots on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: selenium-screenshots
          path: web_ui/tests/screenshots/
          retention-days: 7

      - name: Upload test logs on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: test-logs
          path: |
            web_ui/logs/
            geckodriver.log
            chromedriver.log
          retention-days: 7

  selenium-performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: selenium-e2e-tests
    if: github.event_name == 'push' || github.event.inputs.verbose == 'true'

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb chromium-browser chromium-chromedriver

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -r web_ui/requirements.txt
          pip install selenium pytest-xvfb pytest-django factory-boy pytest-benchmark

      - name: Set up test environment
        working-directory: web_ui
        run: |
          export DJANGO_SETTINGS_MODULE=test_settings
          python manage.py collectstatic --noinput --settings=test_settings
          python manage.py migrate --settings=test_settings

      - name: Start virtual display
        run: |
          export DISPLAY=:99
          Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
          sleep 3

      - name: Run Performance Tests
        working-directory: web_ui
        env:
          DISPLAY: :99
          DJANGO_SETTINGS_MODULE: test_settings
        run: |
          echo "Running performance tests..."
          python manage.py test tests.frontend.test_metadata_selenium \
            -k "performance" \
            --keepdb --verbosity=1 || echo "Performance tests completed with warnings"

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: |
            web_ui/benchmark-results.json
            web_ui/histogram/
          retention-days: 30

  summary:
    runs-on: ubuntu-latest
    needs: [selenium-e2e-tests, selenium-performance-tests]
    if: always()
    
    steps:
      - name: Test Results Summary
        run: |
          echo "## E2E Selenium Test Results 🎪" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.selenium-e2e-tests.result }}" == "success" ]]; then
            echo "✅ **Core E2E Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Core E2E Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.selenium-performance-tests.result }}" == "success" ]]; then
            echo "✅ **Performance Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.selenium-performance-tests.result }}" == "skipped" ]]; then
            echo "⏭️ **Performance Tests**: SKIPPED" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Performance Tests**: COMPLETED WITH WARNINGS" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Coverage" >> $GITHUB_STEP_SUMMARY
          echo "- 🏷️ Tags Management" >> $GITHUB_STEP_SUMMARY
          echo "- 🗂️ Domains Management" >> $GITHUB_STEP_SUMMARY
          echo "- 📖 Glossary Management" >> $GITHUB_STEP_SUMMARY
          echo "- 📦 Data Products Management" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Assertions Management" >> $GITHUB_STEP_SUMMARY
          echo "- 🔗 Cross-page Integration Workflows" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.selenium-e2e-tests.result }}" != "success" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### 🔍 Debugging Help" >> $GITHUB_STEP_SUMMARY
            echo "- Check uploaded screenshots in the Artifacts section" >> $GITHUB_STEP_SUMMARY
            echo "- Review test logs for detailed error information" >> $GITHUB_STEP_SUMMARY
            echo "- Run tests locally with: \`python manage.py test tests.frontend.test_metadata_selenium --verbosity=2\`" >> $GITHUB_STEP_SUMMARY
          fi 